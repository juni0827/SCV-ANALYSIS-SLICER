# dsl2code.py
# Convert DSL token sequence to executable Python code with dynamic generation

import textwrap
from datetime import datetime


class DSLHandler:
    """DSL tokenì„ Processingí•˜ê³  ë™ì ìœ¼ë¡œ ì½”ë“œë¥¼ Createí•˜ëŠ” Handler"""

    @staticmethod
    def _get_basic_stats(df_name="df"):
        return f"{df_name}.describe()"

    @staticmethod
    def _get_info(df_name="df"):
        return f"{df_name}.info()"

    @staticmethod
    def _get_missing_values(df_name="df"):
        return f"{df_name}.isnull().sum()"

    @staticmethod
    def _get_correlation_heatmap(df_name="df"):
        return textwrap.dedent(
            f"""
            import seaborn as sns
            import matplotlib.pyplot as plt
            
            plt.figure(figsize=(10, 8))
            sns.heatmap({df_name}.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt='.2f')
            plt.title('Correlation Heatmap')
            plt.show()
        """
        ).strip()

    @staticmethod
    def _get_distribution_plot(df_name="df", col_idx=0):
        return textwrap.dedent(
            f"""
            import seaborn as sns
            import matplotlib.pyplot as plt
            
            target_col = {df_name}.columns[{col_idx}]
            if pd.api.types.is_numeric_dtype({df_name}[target_col]):
                plt.figure(figsize=(10, 6))
                sns.histplot({df_name}[target_col], kde=True)
                plt.title(f'Distribution of {{target_col}}')
                plt.show()
            else:
                print(f'{{target_col}} is not numeric, skipping histogram.')
        """
        ).strip()

    @staticmethod
    def _get_advanced_combinations(df_name="df"):
        return textwrap.dedent(
            f"""
            from src.core.combinations import AdvancedCombinationsAnalyzer
            analyzer = AdvancedCombinationsAnalyzer()
            analyzer.analyze_all_combinations({df_name})
        """
        ).strip()

    # --- í™•ì¥ëœ Advanced analysis Feature (C51 ~ C70) ---

    @staticmethod
    def _get_time_series_analysis(df_name="df"):
        """C51: ì‹œê³„ì—´ ë¶„ì„ (ë‚ ì§œ Column Automatic ê°ì§€)"""
        return textwrap.dedent(
            f"""
            # ë‚ ì§œ Column Automatic ê°ì§€ ë° ì‹œê³„ì—´ ë¶„ì„
            date_cols = {df_name}.select_dtypes(include=['datetime64']).columns
            if len(date_cols) == 0:
                # ë¬¸ìì—´ì—ì„œ ë‚ ì§œ ë³€í™˜ ì‹œë„
                for col in {df_name}.select_dtypes(include='object').columns:
                    try:
                        {df_name}[col] = pd.to_datetime({df_name}[col])
                        date_cols = {df_name}.select_dtypes(include=['datetime64']).columns
                        print(f"Converted {{col}} to datetime")
                    except:
                        pass
            
            if len(date_cols) > 0:
                target_date = date_cols[0]
                print(f"Analyzing time series on {{target_date}}")
                {df_name}.set_index(target_date)[{df_name}.select_dtypes(include='number').columns[0]].plot(figsize=(12, 6))
                plt.title(f'Time Series Trend')
                plt.show()
            else:
                print("No datetime columns found for time series analysis")
        """
        ).strip()

    @staticmethod
    def _get_outlier_detection(df_name="df"):
        """C52: IQR based ì´ìƒì¹˜ íƒì§€"""
        return textwrap.dedent(
            f"""
            # IQR ë°©ì‹ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€
            numeric_cols = {df_name}.select_dtypes(include='number').columns
            for col in numeric_cols:
                Q1 = {df_name}[col].quantile(0.25)
                Q3 = {df_name}[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = {df_name}[({df_name}[col] < (Q1 - 1.5 * IQR)) | ({df_name}[col] > (Q3 + 1.5 * IQR))]
                if len(outliers) > 0:
                    print(f"Column {{col}}: {{len(outliers)}} outliers detected")
        """
        ).strip()

    @staticmethod
    def _get_pca_analysis(df_name="df"):
        """C53: PCA ì°¨ì› ì¶•ì†Œ ë° Visualization"""
        return textwrap.dedent(
            f"""
            from sklearn.decomposition import PCA
            from sklearn.preprocessing import StandardScaler
            
            # ìˆ˜ì¹˜í˜• Dataë§Œ Optional ë° ê²°ì¸¡ì¹˜ Remove
            numeric_df = {df_name}.select_dtypes(include='number').dropna()
            if len(numeric_df.columns) >= 2:
                scaler = StandardScaler()
                scaled_data = scaler.fit_transform(numeric_df)
                
                pca = PCA(n_components=2)
                pca_result = pca.fit_transform(scaled_data)
                
                plt.figure(figsize=(10, 8))
                plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)
                plt.title('PCA Result (2 Components)')
                plt.xlabel(f'PC1 ({{pca.explained_variance_ratio_[0]:.2%}})')
                plt.ylabel(f'PC2 ({{pca.explained_variance_ratio_[1]:.2%}})')
                plt.show()
            else:
                print("Not enough numeric columns for PCA")
        """
        ).strip()

    @staticmethod
    def _get_text_analysis(df_name="df"):
        """C54: í…ìŠ¤íŠ¸ Column ì›Œë“œí´ë¼ìš°ë“œ"""
        return textwrap.dedent(
            f"""
            from wordcloud import WordCloud
            import matplotlib.pyplot as plt
            
            text_cols = {df_name}.select_dtypes(include='object').columns
            if len(text_cols) > 0:
                text_data = ' '.join({df_name}[text_cols[0]].dropna().astype(str).tolist())
                wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)
                
                plt.figure(figsize=(10, 5))
                plt.imshow(wordcloud, interpolation='bilinear')
                plt.axis('off')
                plt.title(f'Word Cloud for {{text_cols[0]}}')
                plt.show()
            else:
                print("No text columns found for Word Cloud")
        """
        ).strip()

    @staticmethod
    def _get_cluster_analysis(df_name="df"):
        """C55: K-Means í´ëŸ¬ìŠ¤í„°ë§"""
        return textwrap.dedent(
            f"""
            from sklearn.cluster import KMeans
            from sklearn.preprocessing import StandardScaler
            
            numeric_df = {df_name}.select_dtypes(include='number').dropna()
            if len(numeric_df) > 0:
                scaler = StandardScaler()
                scaled_data = scaler.fit_transform(numeric_df)
                
                kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
                clusters = kmeans.fit_predict(scaled_data)
                
                print("Cluster Centers:")
                print(pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=numeric_df.columns))
                
                # Visualization (ì²« ë‘ Column ê¸°ì¤€)
                if len(numeric_df.columns) >= 2:
                    plt.scatter(numeric_df.iloc[:, 0], numeric_df.iloc[:, 1], c=clusters, cmap='viridis')
                    plt.title('K-Means Clustering Result')
                    plt.xlabel(numeric_df.columns[0])
                    plt.ylabel(numeric_df.columns[1])
                    plt.show()
            else:
                print("Not enough numeric data for clustering")
        """
        ).strip()

    @staticmethod
    def _get_smart_visualization(df_name="df"):
        """C60: Data Typeì— according to ìŠ¤ë§ˆíŠ¸ Visualization ì¶”ì²œ"""
        return textwrap.dedent(
            f"""
            # ìŠ¤ë§ˆíŠ¸ Visualization: Data Type Automatic ê°ì§€ ë° ìµœì  ê·¸ë˜í”„ Create
            num_cols = {df_name}.select_dtypes(include='number').columns
            cat_cols = {df_name}.select_dtypes(include='object').columns
            
            print(f"Smart Viz: Found {{len(num_cols)}} numeric and {{len(cat_cols)}} categorical columns")
            
            # Case 1: ìˆ˜ì¹˜í˜• 1ê°œ (íˆìŠ¤í† ê·¸ë¨ + ë°•ìŠ¤í”Œë¡¯)
            if len(num_cols) >= 1:
                target = num_cols[0]
                fig, ax = plt.subplots(1, 2, figsize=(12, 5))
                sns.histplot({df_name}[target], kde=True, ax=ax[0])
                ax[0].set_title(f'Histogram of {{target}}')
                sns.boxplot(x={df_name}[target], ax=ax[1])
                ax[1].set_title(f'Boxplot of {{target}}')
                plt.tight_layout()
                plt.show()
                
            # Case 2: ë²”ì£¼í˜• 1ê°œ (ë§‰ëŒ€ ì°¨íŠ¸)
            if len(cat_cols) >= 1:
                target = cat_cols[0]
                top_n = {df_name}[target].value_counts().head(10)
                plt.figure(figsize=(10, 5))
                sns.barplot(x=top_n.index, y=top_n.values)
                plt.title(f'Top 10 Categories in {{target}}')
                plt.xticks(rotation=45)
                plt.show()
                
            # Case 3: ìˆ˜ì¹˜í˜• 2ê°œ (ì‚°ì ë„ + íšŒê·€ì„ )
            if len(num_cols) >= 2:
                x, y = num_cols[0], num_cols[1]
                plt.figure(figsize=(8, 6))
                sns.regplot(data={df_name}, x=x, y=y, scatter_kws={{'alpha':0.5}})
                plt.title(f'Scatter Plot: {{x}} vs {{y}}')
                plt.show()
                
            # Case 4: ë²”ì£¼í˜• 2ê°œ (íˆíŠ¸ë§µ)
            if len(cat_cols) >= 2:
                c1, c2 = cat_cols[0], cat_cols[1]
                ct = pd.crosstab({df_name}[c1], {df_name}[c2])
                plt.figure(figsize=(10, 8))
                sns.heatmap(ct, annot=True, fmt='d', cmap='YlGnBu')
                plt.title(f'Heatmap: {{c1}} vs {{c2}}')
                plt.show()
        """
        ).strip()

    @staticmethod
    def _get_pie_chart(df_name="df"):
        """C61: ë²”ì£¼í˜• Data ì›í˜• ì°¨íŠ¸"""
        return textwrap.dedent(
            f"""
            # ì›í˜• ì°¨íŠ¸ (Pie Chart)
            cat_cols = {df_name}.select_dtypes(include='object').columns
            if len(cat_cols) > 0:
                target = cat_cols[0]
                counts = {df_name}[target].value_counts()
                
                # í•­ëª©ì´ ë§ìœ¼ë©´ ìƒìœ„ 9ê°œ + Othersë¡œ ë¬¶ê¸°
                if len(counts) > 10:
                    top_9 = counts[:9]
                    others = pd.Series([counts[9:].sum()], index=['Others'])
                    counts = pd.concat([top_9, others])
                
                plt.figure(figsize=(8, 8))
                plt.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('pastel'))
                plt.title(f'Pie Chart of {{target}}')
                plt.show()
            else:
                print("No categorical columns found for Pie Chart")
        """
        ).strip()


# Token ì •ì˜ ë° Handler ë§¤í•‘
TOKEN_HANDLERS = {
    # Basic analysis
    "C1": lambda: "df.describe()",
    "C2": lambda: "df.info()",
    "C3": lambda: "df.isnull().sum()",
    "C4": lambda: "df.dtypes",
    "C5": lambda: "df.nunique()",
    "C6": lambda: "df.head()",
    "C7": lambda: "df.tail()",
    "C8": lambda: "df.corr(numeric_only=True)",
    "C9": lambda: "df.columns.tolist()",
    "C10": lambda: "df.memory_usage(deep=True)",
    # ì¤‘ê¸‰ ë¶„ì„
    "C11": lambda: "(df.isnull().sum() / len(df) * 100).round(2)",
    "C12": DSLHandler._get_correlation_heatmap,
    "C13": lambda: "df[df.columns[0]].value_counts()",
    "C14": lambda: "df.describe(include='all')",
    "C15": lambda: "print(f'Shape: {df.shape}')",
    "C16": lambda: "df.duplicated().sum()",
    "C17": lambda: "df.sample(min(10, len(df)))",
    "C18": lambda: "{col: df[col].unique()[:10] for col in df.columns}",  # ë„ˆë¬´ ê¸¸ì–´ì§ˆ ìˆ˜ ìˆì–´ 10ê°œë¡œ ì œí•œ
    "C19": lambda: "df.head().T",
    "C20": lambda: "df.index",
    # Data ì¡°ì‘ ë° Filterë§
    "C21": lambda: "df[df.isnull().any(axis=1)].head()",
    "C22": lambda: "df.mode().iloc[0]",
    "C23": lambda: "df.hist(figsize=(12, 10)); plt.show()",
    "C24": lambda: "df.select_dtypes(include='object').describe()",
    "C25": lambda: "df.corr(numeric_only=True).unstack().sort_values(ascending=False).drop_duplicates().head(10)",
    "C26": lambda: "df.groupby(df.columns[0]).mean(numeric_only=True)",
    "C27": lambda: "df.to_excel('output.xlsx', index=False)",
    "C28": lambda: "df.to_json('output.json', orient='records')",
    "C29": lambda: "df.std(numeric_only=True)",
    "C30": lambda: "df.agg(['min', 'max'])",
    # ê³ ê¸‰ Statistics ë° Visualization
    "C31": lambda: "(df == 0).sum()",
    "C32": lambda: "df[df.duplicated()]",
    "C33": lambda: "df.notnull().sum()",
    "C34": lambda: "df.index.is_unique",
    "C35": lambda: "sns.pairplot(df.select_dtypes(include='number').dropna().sample(min(100, len(df)))); plt.show()",
    "C36": lambda: "df.sort_values(by=df.columns[0])",
    "C37": lambda: "df.sort_values(by=df.columns[0], ascending=False)",
    "C38": lambda: "f'{df.memory_usage(deep=True).sum() / 1024**2:.2f} MB'",
    "C39": lambda: "pd.concat([df.dtypes, df.isnull().sum()], axis=1, keys=['Type', 'Nulls'])",
    "C40": lambda: "(df.select_dtypes(include='number') < 0).sum()",
    # ì‹¬í™” ë¶„ì„ (C41-C50)
    "C41": lambda: "df.skew(numeric_only=True)",
    "C42": lambda: "df.kurtosis(numeric_only=True)",
    "C43": lambda: "df.quantile([0.25, 0.5, 0.75], numeric_only=True)",
    "C44": lambda: "df.select_dtypes(include='number').mode().iloc[0]",
    "C45": lambda: "(df.nunique() / len(df) * 100).round(2)",
    "C46": lambda: "df.apply(lambda x: x.duplicated().sum())",
    "C47": lambda: "df.boxplot(figsize=(12, 6)); plt.xticks(rotation=45); plt.show()",
    "C48": lambda: "df.columns[df.isnull().any()].tolist()",
    "C49": lambda: "pd.crosstab(df.iloc[:, 0], df.iloc[:, 1]) if len(df.columns) > 1 else 'Not enough columns'",
    "C50": DSLHandler._get_advanced_combinations,
    # --- í™•ì¥ëœ Feature (C51-C70) ---
    "C51": DSLHandler._get_time_series_analysis,
    "C52": DSLHandler._get_outlier_detection,
    "C53": DSLHandler._get_pca_analysis,
    "C54": DSLHandler._get_text_analysis,
    "C55": DSLHandler._get_cluster_analysis,
    "C56": lambda: "df.corr(method='spearman', numeric_only=True)",  # ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜
    "C57": lambda: "df.corr(method='kendall', numeric_only=True)",  # ì¼„ë‹¬ ìƒê´€ê³„ìˆ˜
    "C58": lambda: "df.select_dtypes(include='number').var()",  # ë¶„ì‚°
    "C59": lambda: "df.select_dtypes(include='number').sem()",  # í‘œì¤€ì˜¤ì°¨
    "C60": DSLHandler._get_smart_visualization,  # ìŠ¤ë§ˆíŠ¸ Visualization ì¶”ì²œ
    "C61": DSLHandler._get_pie_chart,  # ì›í˜• ì°¨íŠ¸
    "SAVE": lambda: "# Result Save ë¡œì§ (Execution í™˜ê²½ì— ë”°ë¼ ë‹¤ë¦„)",
    "EXPORT": lambda: "df.to_csv('analysis_result.csv', index=False)",
    "PROFILE": lambda: "import ydata_profiling; ydata_profiling.ProfileReport(df).to_file('report.html')",
}


def _get_token_description(token):
    """Token ì„¤ëª… Return (í™•ì¥ë¨)"""
    descriptions = {
        "C1": "ê¸°ìˆ í†µê³„ ìš”ì•½",
        "C2": "ë°ì´í„° ì •ë³´",
        "C3": "ê²°ì¸¡ì¹˜ ê°œìˆ˜",
        "C4": "ë°ì´í„° íƒ€ì…",
        "C5": "ê³ ìœ ê°’ ê°œìˆ˜",
        "C6": "ìƒìœ„ 5í–‰",
        "C7": "í•˜ìœ„ 5í–‰",
        "C8": "ìƒê´€ê´€ê³„ í–‰ë ¬",
        "C9": "ì»¬ëŸ¼ ëª©ë¡",
        "C10": "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰",
        "C11": "ê²°ì¸¡ì¹˜ ë¹„ìœ¨",
        "C12": "ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ",
        "C13": "ì²« ì»¬ëŸ¼ ê°’ ë¶„í¬",
        "C14": "ìƒì„¸ ê¸°ìˆ í†µê³„",
        "C15": "ë°ì´í„° í¬ê¸°(Shape)",
        "C16": "ì¤‘ë³µí–‰ ê°œìˆ˜",
        "C17": "ëœë¤ ìƒ˜í”Œë§",
        "C18": "ì»¬ëŸ¼ë³„ ê³ ìœ ê°’ ì˜ˆì‹œ",
        "C19": "ë°ì´í„° ì „ì¹˜(Transpose)",
        "C20": "ì¸ë±ìŠ¤ ì •ë³´",
        "C21": "ê²°ì¸¡ì¹˜ í¬í•¨ í–‰ ì¡°íšŒ",
        "C22": "ìµœë¹ˆê°’(Mode)",
        "C23": "íˆìŠ¤í† ê·¸ë¨ ì‹œê°í™”",
        "C24": "ë²”ì£¼í˜• ë³€ìˆ˜ ìš”ì•½",
        "C25": "ì£¼ìš” ìƒê´€ê´€ê³„ ìŒ",
        "C26": "ê·¸ë£¹ë³„ í‰ê· ",
        "C27": "ì—‘ì…€ ì €ì¥",
        "C28": "JSON ì €ì¥",
        "C29": "í‘œì¤€í¸ì°¨",
        "C30": "ìµœëŒ€/ìµœì†Œê°’",
        "C31": "0ì¸ ê°’ ê°œìˆ˜",
        "C32": "ì¤‘ë³µ ë°ì´í„° ì¡°íšŒ",
        "C33": "ìœ íš¨ ë°ì´í„° ê°œìˆ˜",
        "C34": "ê³ ìœ  ì¸ë±ìŠ¤ ì—¬ë¶€",
        "C35": "Pairplot ì‹œê°í™”",
        "C36": "ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬",
        "C37": "ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬",
        "C38": "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰(MB)",
        "C39": "ë°ì´í„° í’ˆì§ˆ ìš”ì•½",
        "C40": "ìŒìˆ˜ê°’ ê°œìˆ˜",
        "C41": "ì™œë„(Skewness)",
        "C42": "ì²¨ë„(Kurtosis)",
        "C43": "4ë¶„ìœ„ìˆ˜",
        "C44": "ìˆ˜ì¹˜í˜• ìµœë¹ˆê°’",
        "C45": "ê³ ìœ ê°’ ë¹„ìœ¨",
        "C46": "ì»¬ëŸ¼ë³„ ì¤‘ë³µë„",
        "C47": "ë°•ìŠ¤í”Œë¡¯",
        "C48": "ê²°ì¸¡ ì»¬ëŸ¼ ëª©ë¡",
        "C49": "êµì°¨í‘œ(Crosstab)",
        "C50": "ê³ ê¸‰ ì¡°í•© ë¶„ì„",
        # í™•ì¥ëœ ì„¤ëª…
        "C51": "ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„",
        "C52": "ì´ìƒì¹˜(Outlier) íƒì§€",
        "C53": "PCA ì°¨ì› ì¶•ì†Œ",
        "C54": "ì›Œë“œí´ë¼ìš°ë“œ(í…ìŠ¤íŠ¸)",
        "C55": "K-Means í´ëŸ¬ìŠ¤í„°ë§",
        "C56": "ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜",
        "C57": "ì¼„ë‹¬ ìƒê´€ê³„ìˆ˜",
        "C58": "ë¶„ì‚°(Variance)",
        "C59": "í‘œì¤€ì˜¤ì°¨(SEM)",
        "C60": "ìŠ¤ë§ˆíŠ¸ ì‹œê°í™” ì¶”ì²œ",
        "C61": "ì›í˜• ì°¨íŠ¸(Pie Chart)",
    }
    return descriptions.get(token, f"ë¶„ì„ ì‘ì—… ({token})")


def dsl_to_code(dsl_sequence, csv_path="your_file.csv"):
    """
    DSL token ì‹œí€€ìŠ¤ë¥¼ Execution Availableí•œ Python ì½”ë“œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    Jinja2 ì—†ì´ë„ ë™ì ì¸ Code generationì„ Supportí•©ë‹ˆë‹¤.
    """
    # Header Create
    lines = [
        "#!/usr/bin/env python3",
        '"""',
        f"Automatic Createëœ ê³ ê¸‰ Data ë¶„ì„ ì½”ë“œ",
        f'DSL ì‹œí€€ìŠ¤: {" â†’ ".join(dsl_sequence)}',
        f'Create ì‹œê°„: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}',
        '"""',
        "",
        "import pandas as pd",
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "import seaborn as sns",
        "import warnings",
        "warnings.filterwarnings('ignore')",
        "",
    ]

    # Data ë¡œë”©
    lines.extend(
        [
            "# --- Data ë¡œë”© ---",
            f"print('ë°ì´í„° ë¡œë”© ì¤‘: {csv_path}')",
            "try:",
            f"    df = pd.read_csv({repr(csv_path)})",
            "    print(f'ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,}í–‰ Ã— {len(df.columns)}ì—´')",
            "except Exception as e:",
            "    print(f'ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}')",
            "    exit(1)",
            "",
        ]
    )

    # Run analysis ë£¨í”„
    lines.append("# --- ë¶„ì„ Start ---")

    for i, token in enumerate(dsl_sequence, 1):
        handler = TOKEN_HANDLERS.get(token)
        description = _get_token_description(token)

        lines.append(f"\n# [{i}] {token}: {description}")
        lines.append(f"print('\\nğŸ”¹ {i}. {description} ({token})')")
        lines.append("try:")

        if handler:
            # Handlerê°€ Functionë©´ Callí•˜ì—¬ ì½”ë“œ ë¬¸ìì—´ì„ ì–»ê³ , ë¬¸ìì—´if ê·¸as Use
            code_block = handler() if callable(handler) else handler

            # ì½”ë“œ ë¸”ë¡ ë“¤ì—¬Write ì ìš©
            indented_code = textwrap.indent(code_block, "    ")

            # Result Output ë¡œì§ì´ Includeë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ printë¡œ ê°ì‹¸ê¸° (ë‹¨ìˆœ í‘œí˜„ì‹ì¸ ê²½ìš°)
            if (
                "print" not in code_block
                and "plt.show" not in code_block
                and "=" not in code_block
                and len(code_block.split("\n")) == 1
            ):
                lines.append(f"    print({code_block})")
            else:
                lines.append(indented_code)
        else:
            lines.append(f"    print('ì•Œ ìˆ˜ ì—†ëŠ” í† í°: {token}')")

        lines.append("except Exception as e:")
        lines.append(f"    print(f'ì˜¤ë¥˜ ë°œìƒ ({token}): {{e}}')")

    lines.extend(
        ["", "# --- ë¶„ì„ Complete ---", "print('\\nëª¨ë“  ë¶„ì„ì´ Completeë˜ì—ˆìŠµë‹ˆë‹¤.')"]
    )

    return "\n".join(lines)


def generate_analysis_template(analysis_type="basic"):
    """ë¶„ì„ í…œí”Œë¦¿ Create"""
    templates = {
        "basic": ["C2", "C15", "C6", "C3", "C1"],
        "statistical": ["C1", "C14", "C29", "C41", "C42", "C43"],
        "visualization": ["C12", "C23", "C35", "C47"],
        "missing_data": ["C3", "C11", "C21", "C48"],
        "correlation": ["C8", "C12", "C25", "C50"],
        "comprehensive": ["C2", "C15", "C3", "C1", "C8", "C12", "C23", "C50"],
        "advanced_ml": ["C51", "C52", "C53", "C55"],  # ìƒˆë¡œ Addëœ ML í…œí”Œë¦¿
        "text_mining": ["C54"],  # í…ìŠ¤íŠ¸ ë¶„ì„ í…œí”Œë¦¿
    }
    return templates.get(analysis_type, templates["basic"])
